# ðŸ“Š Probabilistic Linear Regression (Least Squares â†” MLE)

A clean, hands-on implementation of **Linear Regression** showing how  
**least squares optimization** and **maximum likelihood estimation (MLE)**  
are actually the *same thing* under Gaussian noise.

Built with **realistic noise** so it doesnâ€™t look like a toy demo.

---

## ðŸ‘€ Whatâ€™s this about?

Linear regression is usually taught in two ways:

- minimize squared error (optimization view)
- maximize likelihood assuming Gaussian noise (probabilistic view)

This repo shows:
> **they land on the exact same solution**

and visualizes *why* that happens.

---

## ðŸ§ª Data Setup

Synthetic data generated from:

\[
y = \theta_0 + \theta_1 x + \varepsilon,
\quad \varepsilon \sim \mathcal{N}(0, \sigma^2)
\]

**Ground truth:**
- Î¸â‚€ (bias): `1.0`
- Î¸â‚ (slope): `3.0`
- Noise variance ÏƒÂ²: `1.0` (intentionally high)
- Samples: `100`

Higher noise = more realistic scatter + visible uncertainty.

---

## ðŸ“ Least Squares (Normal Equation)

We solve linear regression in closed form:

\[
\hat{\theta} = (X^\top X)^{-1} X^\top y
\]

This:
- minimizes squared error
- recovers parameters close to ground truth
- also turns out to be the MLE

---

## ðŸ“‰ Loss & Gradients

The code explicitly defines:
- squared error loss  
- analytical gradient  

So itâ€™s easy to extend this to:
- Gradient Descent
- SGD
- momentum / Adam later

---

## ðŸŽ¯ Probabilistic View (MLE)

Assuming Gaussian noise:

\[
p(y \mid x; \theta)
= \mathcal{N}(y; \theta^\top x, \sigma^2)
\]

Log-likelihood:

\[
\ell(\theta)
= -\frac{n}{2}\log(2\pi\sigma^2)
- \frac{1}{2\sigma^2}\sum (y - X\theta)^2
\]

Maximizing this gives the **same Î¸** as least squares.

---

## ðŸ” Estimating Noise (ÏƒÂ²)

Instead of cheating with the true noise, we estimate it:

\[
\hat{\sigma}^2 = \frac{1}{n} \sum (y - X\hat{\theta})^2
\]

This matches how things work in real datasets.

---

## ðŸŒ„ Cost vs Likelihood (Visual Proof)

The repo visualizes:
- squared error surface \( J(\theta) \)
- negative log-likelihood surface \( -\ell(\theta) \)

Even with high noise:

\[
\arg\min J(\theta)
=
\arg\max \ell(\theta)
\]

Different math, same answer.

---

## ðŸ“ Parameter Uncertainty

We compute **95% confidence intervals**:

\[
\text{Var}(\hat{\theta}) = \hat{\sigma}^2 (X^\top X)^{-1}
\]

Higher noise â‡’ wider intervals â‡’ more honest uncertainty.

---

## ðŸ“ˆ Predictive Uncertainty

The model also outputs **predictive intervals**, not just a single line.

This shows:
- where predictions are confident
- where the model is guessing more

Much closer to how regression is used in practice.

---

## âœ… Takeaways

- Least squares = MLE under Gaussian noise
- Noise doesnâ€™t break theory, it exposes uncertainty
- Confidence intervals matter
- Predictive uncertainty matters more
- Linear regression is deeper than it looks

---

## ðŸš€ Things you can extend next

- Gradient Descent / SGD
- MAP estimation with priors
- Bayesian Linear Regression
- Higher-dimensional features
- Logistic regression

---

## ðŸ› ï¸ Tech Stack

- Python 3
- NumPy
- Matplotlib

---

## ðŸ“ Notes

This repo is meant for:
- ML fundamentals
- intuition > formulas
- interview prep
- building blocks for probabilistic ML

---

*Linear Regression, but actually explained.*
